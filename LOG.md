# 100DayLearningLog


## Day 1
* Basics of Pandas and Numpy
* Creating and training the model
* Learnt and Implemented the basics of [Linear Regression](https://github.com/Aman9026/100DaysOfMachineLearning/tree/master/Regression/INFO.md)

## Day 2
* Learnt and Implemented more [Linear Regression](https://github.com/Aman9026/100DaysOfMachineLearning/tree/master/Regression/INFO.md)
* Created graph for visualization


## Day 3
* Learnt and Implemented prediction graphs
* Created graph for visualization

## Day 4
* Created a ML model and integrated it with a basic python program
* Created graph for visualization

## Day 5
* Learnt about [Multi-Linear Regression](https://github.com/Aman9026/100DaysOfMachineLearning/tree/master/Regression/INFO.md)
* Learnt about Feature selection
* Learnt about MAE, MSE and RMS

## Day 6
* Learnt more about Feature Selecton
* Learnt about Correlation
* Learnt more about [Multi-Linear Regression](https://github.com/Aman9026/100DaysOfMachineLearning/tree/master/Regression/INFO.md)

## Day 7
* Learnt more about the coefficient and feature selection
* Created a [Multi-Linear Regression](https://github.com/Aman9026/100DaysOfMachineLearning/tree/master/Regression/INFO.md)

## Day 8

Learnt:
 Significance of negative Co-efficient. More discussion on Co-relation method. Depth discussion on Embedded method(co-efficiency and lesso method/L1 regularization).Label and One-Hot Encoding. Feature Engineering. Categorical Variables. Dummy Variables. Pandas DataFrame. Dummy variable Trap. Multi-co-linearity. Redundant Variables.
Implemented:
 Using SelectFromModel(Lasso(alpha=0.01)) creating lesso model and using .get_support observing features. Finding dummy variables using pandas(pd.get_dummies(state)).Row wise operation in DataFrame using iloc function. After lesso modelling finding the co-efficiency to see which features are important.

## Day 9


Learnt:

 Dimensionality Reduction, Depth discussion on Wrapper Method, Feature Extraction, Principle Component Analysis, OLS method to find important features, Importance of P-Value and Adjusted R-Square, Backward Elimination. 
Implemented:
 Converting categorical values in dummy variable using one single command pd.get_dummies(X,drop_first=True). Creating OLS model using sm.OLS(endog=y,exog=X ).fit(). Manually backward elimination by observing P-Value and Adjusted R-Square. Printing elements from a list using for loop. Creating function using "def" keyword. Passing arguments inside a function. Use of return() keyword in function.


Learnt:
Use of gretl software.Mathematical logic behind p-value.Null
and Alternative Hypothesis.Sample Testing.Traditional
ML(sklearn) vs Modern ML(neural network).Brief overview of
BigData.

 Use of OLS method in gretl.Converting categorical values in dummy variables,backward elimination,visualization etc using gretl.
Face recognition in 3 steps-
i>taking the sample of face using haar cascade classifier from live webcam
ii>Training the samples using LBPH module
iii>Prediction of face in Live footage and also showing the accuracy on screen and authenticating the RHEL system login
